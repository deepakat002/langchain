from langchain_core.prompts import ChatPromptTemplate

# Our news article content (same as before)
news_article_content = """
Scientists at the Mars Rover mission announced today a significant discovery of ancient microbial life evidence.
The latest geological survey of the Jezero Crater revealed unique mineral formations and isotopic signatures
that strongly suggest the presence of water and biological activity billions of years ago. This finding marks
a monumental step in the search for extraterrestrial life and could reshape our understanding of the universe.
"""

# Define the ChatPromptTemplate using the tuple format as requested:
# === ChatPromptTemplate ===
chat_summary_prompt_template = ChatPromptTemplate([
    # System message: Role "system", content is the instruction string
    ("system", "You are an expert news summarizer. Your task is to provide a concise summary, strictly limited to 50 words, ensuring accuracy and neutrality."),
    # Human message: Role "human", content is the template string with placeholder
    ("human", "Here is the news article to summarize:\n{article_text}")
])


# How it's used to create the final list of messages (remains the same):
final_chat_messages = chat_summary_prompt_template.invoke({"article_text": news_article_content})

print(f"--------------final chat messages ----------------\n\n {final_chat_messages} \n\n type:{type(final_chat_messages)}")

print("\n--- Prompt created with ChatPromptTemplate (Tuple Format) ---")
for message in final_chat_messages.messages:  # <-- Corrected
    print(f"Role: {message.type.capitalize()}, Content: {message.content[:150]}...")

print(f"Type of output: {type(final_chat_messages)}")

# You would then typically pass this to a ChatModel like ChatOpenAI:
# from langchain_openai import ChatOpenAI
# from langchain_core.output_parsers import StrOutputParser
#
# model = ChatOpenAI(temperature=0.0)
# parser = StrOutputParser()
#
# chain = chat_summary_prompt_template | model | parser
#
# print("\n--- Summary generated by the LLM Chain (Example if integrated) ---")
# summary_output = chain.invoke({"article_text": news_article_content})
# print(summary_output)
